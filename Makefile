# Dfusion AI Backend - Docker Management

.PHONY: help dev dev-full test ci up down clean logs wait-services setup-ollama

help: ## Show this help message
	@echo "Dfusion AI Backend - Docker Commands"
	@echo ""
	@echo "Available commands:"
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-20s\033[0m %s\n", $$1, $$2}'

up: ## Start all services for development
	@echo "üöÄ Starting development environment..."
	@if [ "$$(uname -m)" = "arm64" ] && [ "$$(uname -s)" = "Darwin" ]; then \
		echo "üì± Detected Apple Silicon, using linux/amd64 platform..."; \
		DOCKER_DEFAULT_PLATFORM=linux/amd64 docker compose up -d; \
	else \
		docker compose up -d; \
	fi
	@echo "‚úÖ Services started!"
	@echo "üìä Qdrant UI: http://localhost:6333/dashboard"
	@echo "üìß MailDev UI: http://localhost:1080"
	@echo "ü§ñ Ollama API: http://localhost:11434"
	@echo "üóÑÔ∏è Adminer: http://localhost:8080"

dev: up ## Alias for up command
	@echo "üîß Development environment ready!"

test: ## Run tests with test environment
	@echo "üß™ Starting test environment..."
	@if [ "$$(uname -m)" = "arm64" ] && [ "$$(uname -s)" = "Darwin" ]; then \
		echo "üì± Detected Apple Silicon, using linux/amd64 platform..."; \
		DOCKER_DEFAULT_PLATFORM=linux/amd64 docker compose -f docker-compose.relational.test.yaml up -d; \
	else \
		docker compose -f docker-compose.relational.test.yaml up -d; \
	fi
	@sleep 15
	@echo "üß™ Running tests..."
	npm run test:e2e
	@echo "üõë Stopping test environment..."
	docker compose -f docker-compose.relational.test.yaml down

ci: ## Run full CI pipeline with e2e tests
	@echo "üöÄ Starting CI pipeline..."
	@if [ "$$(uname -m)" = "arm64" ] && [ "$$(uname -s)" = "Darwin" ]; then \
		echo "üì± Detected Apple Silicon, using linux/amd64 platform..."; \
		DOCKER_DEFAULT_PLATFORM=linux/amd64 docker compose -f docker-compose.relational.ci.yaml --env-file env-example-relational -p ci-relational up --build --exit-code-from api; \
	else \
		docker compose -f docker-compose.relational.ci.yaml --env-file env-example-relational -p ci-relational up --build --exit-code-from api; \
	fi

ci-start: ## Start CI environment only
	@echo "üöÄ Starting CI environment..."
	@if [ "$$(uname -m)" = "arm64" ] && [ "$$(uname -s)" = "Darwin" ]; then \
		echo "üì± Detected Apple Silicon, using linux/amd64 platform..."; \
		DOCKER_DEFAULT_PLATFORM=linux/amd64 docker compose -f docker-compose.relational.ci.yaml up -d; \
	else \
		docker compose -f docker-compose.relational.ci.yaml up -d; \
	fi
	@echo "‚úÖ CI environment ready!"

down: ## Stop all services
	@echo "üõë Stopping all services..."
	docker compose down
	docker compose -f docker-compose.relational.test.yaml down 2>/dev/null || true
	docker compose -f docker-compose.relational.ci.yaml down 2>/dev/null || true
	@echo "‚úÖ All services stopped"

clean: ## Stop and remove all containers, volumes, and networks
	@echo "üßπ Cleaning up all Docker resources..."
	docker compose down -v --remove-orphans
	docker compose -f docker-compose.relational.test.yaml down -v --remove-orphans 2>/dev/null || true
	docker compose -f docker-compose.relational.ci.yaml down -v --remove-orphans 2>/dev/null || true
	@echo "‚úÖ Cleanup complete"

logs: ## Show logs for all services
	docker compose logs -f

logs-ollama: ## Show Ollama logs
	docker compose logs -f ollama

logs-qdrant: ## Show Qdrant logs
	docker compose logs -f qdrant

logs-api: ## Show API logs
	docker compose logs -f api

status: ## Show status of all services
	@echo "üìã Service Status:"
	docker compose ps

models: ## List available Ollama models
	@echo "ü§ñ Available Ollama models:"
	@docker exec ollama-embeddings ollama list 2>/dev/null || echo "Ollama not running"

pull-model: ## Pull additional Ollama model (usage: make pull-model MODEL=llama2)
	@if [ -z "$(MODEL)" ]; then echo "‚ùå Usage: make pull-model MODEL=model-name"; exit 1; fi
	@echo "üì• Pulling model: $(MODEL)"
	@docker exec ollama-embeddings ollama pull $(MODEL)

setup-ollama: ## Setup Ollama with embedding model
	@echo "ü§ñ Setting up Ollama with embedding model..."
	@docker exec ollama-embeddings /bin/sh -c "\
		echo 'Waiting for Ollama to be ready...' && \
		sleep 5 && \
		echo 'Pulling embedding model...' && \
		curl -X POST http://localhost:11434/api/pull -d '{\"name\":\"nomic-embed-text\"}' || echo 'Model pull failed, continuing...' && \
		sleep 10 \
	"
	@echo "‚úÖ Ollama setup complete!"

wait-services: ## Wait for all services to be ready
	@echo "‚è≥ Waiting for services to be ready..."
	@echo "Waiting for PostgreSQL..."
	@./wait-for-it.sh localhost:5432 -t 60
	@echo "Waiting for MailDev..."
	@./wait-for-it.sh localhost:1080 -t 60
	@echo "Waiting for Qdrant..."
	@./wait-for-it.sh localhost:6333 -t 60
	@echo "Waiting for Ollama..."
	@./wait-for-it.sh localhost:11434 -t 60
	@make setup-ollama
	@echo "‚úÖ All services ready!"

dev-full: ## Full development setup with service waiting
	@echo "üöÄ Starting full development environment..."
	@make up
	@make wait-services
	@echo "üîß Running migrations and seeds..."
	@npm run migration:run
	@npm run seed:run:relational
	@echo "üéâ Development environment fully ready!"

# Default target
.DEFAULT_GOAL := help